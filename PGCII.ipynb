{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "8622963e-db2e-4d21-8353-5fdcc7da6309",
      "cell_type": "code",
      "source": "pip install opencv-python imutils scipy numpy matplotlib tensorflow==2.12 mediapipe fer opencv-python dlib",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "<class 'OSError'>",
          "evalue": "Not available",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minstall opencv-python imutils scipy numpy matplotlib tensorflow==2.12 mediapipe fer opencv-python dlib\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2480\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2478\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2479\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2480\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2482\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2484\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
            "File \u001b[0;32m/lib/python3.12/site-packages/IPython/core/magics/packaging.py:92\u001b[0m, in \u001b[0;36mPackagingMagics.pip\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     python \u001b[38;5;241m=\u001b[39m shlex\u001b[38;5;241m.\u001b[39mquote(python)\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpython\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-m\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNote: you may need to restart the kernel to use updated packages.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2653\u001b[0m, in \u001b[0;36mInteractiveShell.system_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m   2648\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackground processes not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2650\u001b[0m \u001b[38;5;66;03m# we explicitly do NOT return the subprocess status code, because\u001b[39;00m\n\u001b[1;32m   2651\u001b[0m \u001b[38;5;66;03m# a non-None value would trigger :func:`sys.displayhook` calls.\u001b[39;00m\n\u001b[1;32m   2652\u001b[0m \u001b[38;5;66;03m# Instead, we store the exit_code in user_ns.\u001b[39;00m\n\u001b[0;32m-> 2653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar_expand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/lib/python3.12/site-packages/IPython/utils/_process_emscripten.py:11\u001b[0m, in \u001b[0;36msystem\u001b[0;34m(cmd)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msystem\u001b[39m(cmd):\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot available\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mOSError\u001b[0m: Not available"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 2
    },
    {
      "id": "9cb10ca2-a753-4c91-bc9b-a60e1edc6b84",
      "cell_type": "code",
      "source": "import cv2\nimport dlib\n\n# Teste OpenCV\nprint(f\"OpenCV version: {cv2.__version__}\")\n\n# Teste Dlib\ndetector = dlib.get_frontal_face_detector()\nprint(\"Dlib está funcionando corretamente.\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1d26bf7b-3478-4290-8a9a-5175d6dd8ce7",
      "cell_type": "code",
      "source": "import requests\nimport bz2\nimport shutil\n\nurl = \"https://github.com/davisking/dlib-models/raw/master/shape_predictor_68_face_landmarks.dat.bz2\"\noutput_file = \"shape_predictor_68_face_landmarks.dat.bz2\"\n\n# Baixa o arquivo\nresponse = requests.get(url)\nwith open(output_file, 'wb') as file:\n    file.write(response.content)\n\n# Descompacta o arquivoA\nwith bz2.BZ2File(output_file, \"rb\") as source, open(\"shape_predictor_68_face_landmarks.dat\", \"wb\") as dest:\n    shutil.copyfileobj(source, dest)\n\nprint(\"Download e descompactação concluídos.\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6289a804-22b7-4a19-b254-4176277098a5",
      "cell_type": "code",
      "source": "import requests\n\n# URL do arquivo no GitHub\nurl = \"https://github.com/oarriaga/face_classification/raw/master/trained_models/emotion_models/fer2013_mini_XCEPTION.102-0.66.hdf5\"\noutput_file = \"fer2013_mini_XCEPTION.102-0.66.hdf5\"\n\n# Faz o download do arquivo\nresponse = requests.get(url, stream=True)\nwith open(output_file, 'wb') as file:\n    # Escreve o conteúdo em partes para não sobrecarregar a memória\n    shutil.copyfileobj(response.raw, file)\n\nprint(f\"Download do arquivo {output_file} concluído.\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f5236011-e7d7-449f-8c89-278f5761b5fe",
      "cell_type": "code",
      "source": "import cv2\nimport dlib\nimport numpy as np\nfrom imutils import face_utils\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D\nfrom tensorflow.keras.applications import EfficientNetB0\nimport time\nfrom datetime import datetime\nimport pandas as pd\nfrom scipy.spatial import distance\n\nclass EnhancedNeuroAssessmentToolkit:\n    def __init__(self):\n        self.setup_detectors()\n        self.setup_models()\n        self.setup_parameters()\n\n    def setup_detectors(self):\n        \"\"\"Configura detectores faciais e outros sensores\"\"\"\n        self.face_detector = dlib.get_frontal_face_detector()\n        self.landmark_predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n\n    def setup_models(self):\n        \"\"\"Configura modelos especializados para diferentes aspectos\"\"\"\n        base_model = EfficientNetB0(\n            weights='imagenet',\n            include_top=False,\n            input_shape=(224, 224, 3)\n        )\n\n        # Modelo de classificação para expressões faciais\n        self.expression_model = Sequential([\n            base_model,\n            GlobalAveragePooling2D(),\n            Dense(128, activation='relu'),\n            Dense(7, activation='softmax')  # 7 expressões básicas\n        ])\n\n    def setup_parameters(self):\n        \"\"\"Configura parâmetros de análise\"\"\"\n        self.data_buffers = {\n            'gaze': [],\n            'head_movements': [],\n            'expressions': [],\n            'eye_aspect_ratio': []\n        }\n        self.max_buffer_size = 300  # 10 segundos a 30 fps\n\n    def calculate_eye_aspect_ratio(self, eye_landmarks):\n        \"\"\"Calcula razão de aspecto dos olhos para detecção de piscadas\"\"\"\n        v1 = distance.euclidean(eye_landmarks[1], eye_landmarks[5])\n        v2 = distance.euclidean(eye_landmarks[2], eye_landmarks[4])\n        h = distance.euclidean(eye_landmarks[0], eye_landmarks[3])\n        return (v1 + v2) / (2.0 * h)\n\n    def analyze_attention(self, eye_aspect_ratio):\n        \"\"\"Analisa atenção com base na razão de aspecto dos olhos (EAR)\"\"\"\n        # Defina um limite de EAR para considerar que a pessoa está atenta\n        EAR_THRESHOLD = 0.2\n        if eye_aspect_ratio < EAR_THRESHOLD:\n            return \"Potencialmente distraído\"\n        else:\n            return \"Atenção mantida\"\n\n    def analyze_gaze(self, landmarks):\n        \"\"\"Analisa direção e estabilidade do olhar\"\"\"\n        left_eye = landmarks[36:42]\n        right_eye = landmarks[42:48]\n\n        # Calcular centro dos olhos\n        left_center = np.mean(left_eye, axis=0)\n        right_center = np.mean(right_eye, axis=0)\n\n        # Calcular direção do olhar\n        gaze_direction = np.array([\n            right_center[0] - left_center[0],\n            right_center[1] - left_center[1]\n        ])\n\n        return {\n            'direction': gaze_direction,\n            'stability': np.std(gaze_direction)\n        }\n\n    def analyze_head_movements(self, landmarks, prev_landmarks):\n        \"\"\"Analisa movimentos da cabeça\"\"\"\n        if prev_landmarks is None:\n            return {'movement': 0, 'stability': 1.0}\n\n        movement = np.mean(np.abs(landmarks - prev_landmarks))\n        stability = 1.0 / (1.0 + movement)\n\n        return {\n            'movement': movement,\n            'stability': stability\n        }\n\n    def process_frame(self, frame, prev_landmarks=None):\n        \"\"\"Processa um frame com análises expandidas\"\"\"\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        faces = self.face_detector(gray, 0)\n\n        frame_data = {\n            'timestamp': datetime.now(),\n            'gaze_metrics': None,\n            'head_movement': None,\n            'expression': 'neutral',\n            'eye_aspect_ratio': None,\n            'attention': None\n        }\n\n        for face in faces:\n            landmarks = self.landmark_predictor(gray, face)\n            landmarks = face_utils.shape_to_np(landmarks)\n\n            # Análises\n            gaze_metrics = self.analyze_gaze(landmarks)\n            head_metrics = self.analyze_head_movements(landmarks, prev_landmarks)\n            ear = self.calculate_eye_aspect_ratio(landmarks[36:42])  # Olho esquerdo\n            attention = self.analyze_attention(ear)\n\n            # Atualizar dados do frame\n            frame_data.update({\n                'gaze_metrics': gaze_metrics,\n                'head_movement': head_metrics,\n                'eye_aspect_ratio': ear,\n                'attention': attention\n            })\n\n            # Para análise de expressões faciais (se você tiver um método de predição)\n            # expression_pred = self.expression_model.predict(frame)  # Pseudo código\n            # frame_data['expression'] = expression_pred\n\n            return frame, frame_data, landmarks\n\n        return frame, frame_data, None\n\n    def start_assessment(self, duration_seconds=300):\n        \"\"\"Inicia uma sessão de avaliação geral\"\"\"\n        cap = cv2.VideoCapture(0)\n        start_time = time.time()\n        frames_data = []\n        prev_landmarks = None\n\n        try:\n            while (time.time() - start_time) < duration_seconds:\n                ret, frame = cap.read()\n                if not ret:\n                    break\n\n                processed_frame, frame_data, landmarks = self.process_frame(frame, prev_landmarks)\n\n                if frame_data:\n                    frames_data.append(frame_data)\n\n                prev_landmarks = landmarks\n\n                cv2.imshow('Assessment', processed_frame)\n                if cv2.waitKey(1) & 0xFF == ord('q'):\n                    break\n\n        finally:\n            cap.release()\n            cv2.destroyAllWindows()\n\n        return self.generate_report(frames_data)\n\n    def generate_report(self, frames_data):\n        \"\"\"Gera relatório detalhado baseado na avaliação\"\"\"\n        df = pd.DataFrame(frames_data)\n\n        report = {\n            'timestamp': datetime.now(),\n            'average_attention': df['attention'].mode()[0] if not df['attention'].isnull().all() else 'Indeterminado',\n            'average_gaze_direction': df['gaze_metrics'].apply(lambda x: x['direction'] if x is not None else np.nan).mean(),\n            'average_head_movement': df['head_movement'].apply(lambda x: x['movement'] if x is not None else np.nan).mean(),\n            'average_eye_aspect_ratio': df['eye_aspect_ratio'].mean(),\n            'average_expression': df['expression'].mode()[0] if not df['expression'].isnull().all() else 'Indeterminado'\n        }\n\n        return report\n\n# Uso do toolkit\ntoolkit = EnhancedNeuroAssessmentToolkit()\nreport = toolkit.start_assessment(duration_seconds=60)\nprint(report)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0acf2c92-4d01-4196-90ee-a696ea1f01da",
      "cell_type": "code",
      "source": "import cv2\nimport dlib\nimport numpy as np\nfrom imutils import face_utils\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D\nfrom tensorflow.keras.applications import EfficientNetB0\nimport time\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\nclass NeuroAssessmentToolkit:\n    def __init__(self):\n        \"\"\"Inicializa o toolkit com suporte a câmera\"\"\"\n        self.setup_detectors()\n        self.setup_models()\n        self.setup_parameters()\n        \n    def setup_detectors(self):\n        \"\"\"Configura detectores faciais\"\"\"\n        try:\n            self.face_detector = dlib.get_frontal_face_detector()\n            self.landmark_predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n        except Exception as e:\n            print(f\"Erro configurando detectores: {e}\")\n            raise\n            \n    def setup_models(self):\n        \"\"\"Configura modelos de deep learning\"\"\"\n        try:\n            # Modelo de atenção\n            base_model = EfficientNetB0(\n                weights='imagenet',\n                include_top=False,\n                input_shape=(224, 224, 3)\n            )\n            \n            self.attention_model = Sequential([\n                base_model,\n                GlobalAveragePooling2D(),\n                Dense(256, activation='relu'),\n                Dense(3, activation='softmax')  # Três classes de atenção\n            ])\n            \n            # Modelo de emoção (exemplo simplificado)\n            self.emotion_model = Sequential([\n                base_model,\n                GlobalAveragePooling2D(),\n                Dense(256, activation='relu'),\n                Dense(7, activation='softmax')  # Exemplo: 7 emoções básicas\n            ])\n            \n        except Exception as e:\n            print(f\"Erro configurando modelos: {e}\")\n            raise\n            \n    def setup_parameters(self):\n        \"\"\"Configura parâmetros de análise\"\"\"\n        self.attention_buffer = []\n        self.emotion_buffer = []\n        self.head_movement_buffer = []\n        self.gaze_direction_buffer = []\n        self.eye_aspect_ratio_buffer = []\n        self.frame_buffer = []\n        self.max_buffer_size = 30  # 1 segundo a 30 fps\n        \n    def process_frame(self, frame):\n        \"\"\"Processa um frame de vídeo\"\"\"\n        try:\n            # Converter para escala de cinza para detecção facial\n            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n            \n            # Detectar faces\n            faces = self.face_detector(gray, 0)\n            \n            frame_data = {\n                'timestamp': datetime.now(),\n                'attention': 0,\n                'emotion': 'neutral',\n                'landmarks': None,\n                'head_movement': 0,\n                'gaze_direction': None,\n                'eye_aspect_ratio': 0\n            }\n            \n            for face in faces:\n                # Extrair landmarks faciais\n                landmarks = self.landmark_predictor(gray, face)\n                landmarks = face_utils.shape_to_np(landmarks)\n                frame_data['landmarks'] = landmarks\n                \n                # Analisar atenção\n                attention_score = self.analyze_attention(frame, face)\n                frame_data['attention'] = attention_score\n                \n                # Analisar emoção\n                emotion = self.analyze_emotion(frame, face)\n                frame_data['emotion'] = emotion\n                \n                # Analisar movimento da cabeça\n                head_movement = self.analyze_head_movement(landmarks)\n                frame_data['head_movement'] = head_movement\n                \n                # Analisar direção do olhar\n                gaze_direction = self.analyze_gaze(landmarks)\n                frame_data['gaze_direction'] = gaze_direction\n                \n                # Analisar proporção dos olhos\n                eye_aspect_ratio = self.calculate_eye_aspect_ratio(landmarks)\n                frame_data['eye_aspect_ratio'] = eye_aspect_ratio\n                \n                # Desenhar resultados no frame\n                self.draw_analysis(frame, face, landmarks, attention_score, emotion)\n                \n            return frame, frame_data\n            \n        except Exception as e:\n            print(f\"Erro processando frame: {e}\")\n            return frame, None\n            \n    def analyze_attention(self, frame, face):\n        \"\"\"Analisa nível de atenção\"\"\"\n        try:\n            # Extrair ROI da face\n            x, y, w, h = face.left(), face.top(), face.width(), face.height()\n            face_img = frame[y:y+h, x:x+w]\n            \n            # Redimensionar para input do modelo\n            face_img = cv2.resize(face_img, (224, 224))\n            face_img = tf.keras.applications.efficientnet.preprocess_input(face_img)\n            face_img = np.expand_dims(face_img, axis=0)\n            \n            # Predição\n            attention_pred = self.attention_model.predict(face_img, verbose=0)\n            \n            return float(attention_pred[0][0])  # Sustained attention score\n            \n        except Exception as e:\n            print(f\"Erro analisando atenção: {e}\")\n            return 0\n            \n    def analyze_emotion(self, frame, face):\n        \"\"\"Analisa emoção\"\"\"\n        try:\n            # Extrair ROI da face\n            x, y, w, h = face.left(), face.top(), face.width(), face.height()\n            face_img = frame[y:y+h, x:x+w]\n            \n            # Redimensionar para input do modelo\n            face_img = cv2.resize(face_img, (224, 224))\n            face_img = tf.keras.applications.efficientnet.preprocess_input(face_img)\n            face_img = np.expand_dims(face_img, axis=0)\n            \n            # Predição\n            emotion_pred = self.emotion_model.predict(face_img, verbose=0)\n            emotion_labels = ['neutral', 'happy', 'sad', 'angry', 'surprised', 'disgusted', 'fearful']\n            return emotion_labels[np.argmax(emotion_pred)]\n        \n        except Exception as e:\n            print(f\"Erro analisando emoção: {e}\")\n            return 'neutral'\n    \n    def analyze_head_movement(self, landmarks):\n        \"\"\"Analisa o movimento da cabeça\"\"\"\n        # Simples exemplo, implementar lógica para calcular movimento da cabeça\n        # Você pode utilizar a diferença entre landmarks em frames sucessivos\n        return np.random.rand()  # Substitua por cálculo real\n\n    def analyze_gaze(self, landmarks):\n        \"\"\"Analisa a direção do olhar\"\"\"\n        # Simples exemplo, retornar uma direção fixa\n        return np.random.rand(2) * 100  # Substitua por cálculo real\n\n    def calculate_eye_aspect_ratio(self, landmarks):\n        \"\"\"Calcula a proporção dos olhos\"\"\"\n        left_eye = landmarks[36:42]  # Coordenadas dos pontos do olho esquerdo\n        right_eye = landmarks[42:48]  # Coordenadas dos pontos do olho direito\n        \n        # Calcular a altura e largura dos olhos\n        left_eye_aspect_ratio = (np.linalg.norm(left_eye[1] - left_eye[5]) +\n                                  np.linalg.norm(left_eye[2] - left_eye[4])) / (2.0 * np.linalg.norm(left_eye[0] - left_eye[3]))\n        right_eye_aspect_ratio = (np.linalg.norm(right_eye[1] - right_eye[5]) +\n                                   np.linalg.norm(right_eye[2] - right_eye[4])) / (2.0 * np.linalg.norm(right_eye[0] - right_eye[3]))\n\n        return (left_eye_aspect_ratio + right_eye_aspect_ratio) / 2.0\n    \n    def draw_analysis(self, frame, face, landmarks, attention_score, emotion):\n        \"\"\"Desenha resultados da análise no frame\"\"\"\n        try:\n            # Desenhar retângulo da face\n            x, y, w, h = face.left(), face.top(), face.width(), face.height()\n            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n            \n            # Desenhar landmarks\n            for (x, y) in landmarks:\n                cv2.circle(frame, (x, y), 1, (0, 0, 255), -1)\n                \n            # Mostrar scores\n            cv2.putText(frame, f\"Attention: {attention_score:.2f}\", \n                       (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n            cv2.putText(frame, f\"Emotion: {emotion}\", \n                       (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n                       \n        except Exception as e:\n            print(f\"Erro desenhando análise: {e}\")\n            \n    def start_assessment(self, duration_seconds=30):\n        \"\"\"Inicia uma sessão de avaliação usando a câmera\"\"\"\n        try:\n            cap = cv2.VideoCapture(0)\n            \n            if not cap.isOpened():\n                raise Exception(\"Não foi possível acessar a câmera\")\n                \n            start_time = time.time()\n            frames_data = []\n            \n            while (time.time() - start_time) < duration_seconds:\n                ret, frame = cap.read()\n                if not ret:\n                    break\n                    \n                # Processar frame\n                processed_frame, frame_data = self.process_frame(frame)\n                \n                if frame_data:\n                    frames_data.append(frame_data)\n                \n                # Mostrar frame\n                cv2.imshow('Assessment', processed_frame)\n                \n                # Pressione 'q' para sair\n                if cv2.waitKey(1) & 0xFF == ord('q'):\n                    break\n                    \n            cap.release()\n            cv2.destroyAllWindows()\n            \n            # Gerar relatório\n            self.generate_report(frames_data)\n            \n        except Exception as e:\n            print(f\"Erro durante avaliação: {e}\")\n            if 'cap' in locals():\n                cap.release()\n            cv2.destroyAllWindows()\n            \n    def generate_report(self, frames_data):\n        \"\"\"Gera relatório da avaliação\"\"\"\n        try:\n            attention_scores = [f['attention'] for f in frames_data]\n            gaze_directions = [f['gaze_direction'] for f in frames_data]\n            head_movements = [f['head_movement'] for f in frames_data]\n            eye_aspect_ratios = [f['eye_aspect_ratio'] for f in frames_data]\n            emotions = [f['emotion'] for f in frames_data]\n            \n            plt.figure(figsize=(12, 6))\n            plt.plot(attention_scores, label='Attention Levels', color='b')\n            plt.title('Attention Levels During Assessment')\n            plt.xlabel('Frame')\n            plt.ylabel('Attention Score')\n            plt.legend()\n            plt.show()\n            \n            print(\"\\nRelatório da Avaliação:\")\n            print(f\"Duração: {len(frames_data)} frames\")\n            print(f\"Atenção média: {np.mean(attention_scores):.2f}\")\n            print(f\"Variação da atenção: {np.std(attention_scores):.2f}\")\n            print(f\"Direção média do olhar: {np.mean(gaze_directions, axis=0)}\")\n            print(f\"Média do movimento da cabeça: {np.mean(head_movements):.2f}\")\n            print(f\"Proporção média dos olhos: {np.mean(eye_aspect_ratios):.2f}\")\n            print(f\"Emoção mais comum: {max(set(emotions), key=emotions.count)}\")\n            \n        except Exception as e:\n            print(f\"Erro gerando relatório: {e}\")\n\n# Uso do toolkit\ntoolkit = NeuroAssessmentToolkit()\ntoolkit.start_assessment(duration_seconds=30)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1581d072-d5d9-489f-88eb-ab4fa4dbc3b9",
      "cell_type": "code",
      "source": "import cv2\nimport dlib\nimport numpy as np\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\nclass NeuroBehaviorToolkit:\n    def __init__(self):\n        self.detector = dlib.get_frontal_face_detector()\n        self.predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n\n        self.blink_counter = 0\n        self.eye_movements = []\n        self.body_movements = []\n        self.emotions = []\n\n\n    def start_assessment(self, duration_seconds=10):\n        \"\"\"Inicia a avaliação geral por vídeo.\"\"\"\n        cap = cv2.VideoCapture(0)\n        start_time = datetime.now()\n\n        while (datetime.now() - start_time).seconds < duration_seconds:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            self.process_frame(frame)\n            cv2.imshow(\"Avaliação Neuropsicológica\", frame)\n            if cv2.waitKey(1) & 0xFF == ord('q'):\n                break\n\n        cap.release()\n        cv2.destroyAllWindows()\n        self.generate_report()\n\n    def process_frame(self, frame):\n        \"\"\"Processa um frame para detectar piscadas, movimentos e expressões faciais.\"\"\"\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        faces = self.detector(gray)\n\n        for face in faces:\n            landmarks = self.predictor(gray, face)\n\n            # Detecta piscadas\n            if self.is_blinking(landmarks):\n                self.blink_counter += 1\n\n            # Rastreia movimentos oculares\n            eye_center = self.get_eye_center(landmarks, [36, 39])\n            self.eye_movements.append(eye_center)\n\n            # Detecta expressões faciais (emocional)\n            emotion = self.detect_emotion(landmarks)\n            self.emotions.append(emotion)\n\n        # Detecta movimentos corporais\n        movement_intensity = self.detect_body_movement(frame)\n        self.body_movements.append(movement_intensity)\n\n    def is_blinking(self, landmarks):\n        \"\"\"Verifica se o olho está piscando.\"\"\"\n        left_ear = self.calculate_eye_aspect_ratio(landmarks, [36, 37, 38, 39, 40, 41])\n        right_ear = self.calculate_eye_aspect_ratio(landmarks, [42, 43, 44, 45, 46, 47])\n        return left_ear < 0.25 or right_ear < 0.25\n\n    def detect_emotion(self, landmarks):\n        \"\"\"Simples detecção de expressão emocional (exemplo).\"\"\"\n        mouth_aspect_ratio = self.calculate_mouth_aspect_ratio(landmarks, [48, 51, 57, 54])\n        if mouth_aspect_ratio > 0.5:\n            return \"Euforia\"\n        elif mouth_aspect_ratio < 0.3:\n            return \"Tristeza\"\n        return \"Neutro\"\n\n    def calculate_eye_aspect_ratio(self, landmarks, points):\n        \"\"\"Calcula a razão de aspecto do olho (EAR).\"\"\"\n        p1, p2, p3, p4, p5, p6 = [landmarks.part(i) for i in points]\n        horizontal_dist = np.linalg.norm([p4.x - p1.x, p4.y - p1.y])\n        vertical_dist = (np.linalg.norm([p2.x - p6.x, p2.y - p6.y]) +\n                         np.linalg.norm([p3.x - p5.x, p3.y - p5.y])) / 2.0\n        return vertical_dist / horizontal_dist\n\n    def calculate_mouth_aspect_ratio(self, landmarks, points):\n        \"\"\"Calcula a razão de aspecto da boca.\"\"\"\n        p1, p2, p3, p4 = [landmarks.part(i) for i in points]\n        vertical_dist = np.linalg.norm([p2.x - p3.x, p2.y - p3.y])\n        horizontal_dist = np.linalg.norm([p1.x - p4.x, p1.y - p4.y])\n        return vertical_dist / horizontal_dist\n\n    def get_eye_center(self, landmarks, points):\n        \"\"\"Calcula o centro do olho.\"\"\"\n        x = np.mean([landmarks.part(p).x for p in points])\n        y = np.mean([landmarks.part(p).y for p in points])\n        return (x, y)\n\n    def detect_body_movement(self, frame):\n        \"\"\"Detecta movimento corporal.\"\"\"\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        if hasattr(self, 'previous_frame'):\n            diff = cv2.absdiff(self.previous_frame, gray)\n            movement = np.sum(diff) / (frame.shape[0] * frame.shape[1])\n        else:\n            movement = 0\n        self.previous_frame = gray\n        return movement\n\n    def generate_report(self):\n        \"\"\"Gera o relatório com base nos dados coletados.\"\"\"\n        print(\"\\n--- Relatório de Avaliação ---\")\n        print(f\"Piscos Detectados: {self.blink_counter}\")\n        print(f\"Média de Movimento Corporal: {np.mean(self.body_movements):.2f}\")\n\n        if np.mean(self.body_movements) > 0.02:\n            print(\"- Hiperatividade: Movimentos corporais elevados detectados.\")\n\n        print(f\"Expressões Emocionais: {set(self.emotions)}\")\n        if \"Euforia\" in self.emotions and \"Tristeza\" in self.emotions:\n            print(\"- Bipolaridade: Flutuações emocionais extremas detectadas.\")\n\n        self.plot_eye_movements()\n\n    def plot_eye_movements(self):\n        \"\"\"Gera um gráfico dos movimentos oculares.\"\"\"\n        eye_x = [pos[0] for pos in self.eye_movements]\n        eye_y = [pos[1] for pos in self.eye_movements]\n\n        plt.plot(eye_x, label='Movimento Horizontal')\n        plt.plot(eye_y, label='Movimento Vertical')\n        plt.title('Movimentos Oculares')\n        plt.xlabel('Frame')\n        plt.ylabel('Posição')\n        plt.legend()\n        plt.show()\n\n# Executar a avaliação\ntoolkit = NeuroBehaviorToolkit()\ntoolkit.start_assessment(duration_seconds=30)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e31a5a33-80ea-435d-af69-793128a4d11b",
      "cell_type": "code",
      "source": "pip install opencv-python dlib numpy tensorflow tensorflow-probability cma imutils matplotlib\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "822a9d18-1e4d-491e-91d7-a7b58b7a5c7e",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}